{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "#### Author: Dhrruv Tokas\n",
    "#### Email ID: dhrruvtokas@gmail.com\n",
    "\n",
    "Date: 1/7/2020\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* os 0.1.4 (mainly for working with os specific task (reading into directories), included in Anaconda Python 3.6)\n",
    "* regex 2020.7.14 (mainly for, included in Anaconda Python 3.6) \n",
    "* langid 1.1.6 (mainly for language related operations, included in Anaconda Python 3.6) \n",
    "\n",
    "## Phase 1: Parsing Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This task focuses on extracting the `meaningful` data from the available datasets and parsing that data which is collected from several `semi-structured` files. The final `XML` file have around `956` `unique` `tweet-ids` which have been provided in the later half of the environment.\n",
    "\n",
    "Tasks that were performed:\n",
    "1. Data Extraction: Extracting plausible information such as `tweet-ids`, `tweet-texts`, and `tweet-dates` from the provided text files `(.txt)`.\n",
    "2. Parsing Semi-structured text files: Parsing this information into a new `XML` file which have its own specified format.\n",
    "3. Reviewing: Reading final `XML` File\n",
    "\n",
    "Other details are thoroughly provided in the following sections of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries which will be required throughout this task\n",
    "# For this specific task, I have only used 3 different libraries: os ,regex or reg , and langid\n",
    "import os # useful for carrying out operating system related tasks\n",
    "import re # useful for all regular expression operations\n",
    "import langid # useful for language related tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hold and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set() function is used to keep a record of each id that has been parsed into an xml file in order to avoid repeated id values \n",
    "hold_repeated = set() # hold_repeated variable contains set() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading and Understanding The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 # Count is used to keep a record of each file that will be opened for parsing\n",
    "current_directory = os.getcwd() # Current working directory\n",
    "print(\"Dataset: Semi-structured Text Files:\") \n",
    "print(\"\\n\") #Next line\n",
    "for root, directory, file_list in os.walk(current_directory): #Checks the current working directory\n",
    "    for file_name in file_list: #For each file which is present in the current working directory\n",
    "        if file_name.endswith(\".txt\"): #Checks if the name of the file ends with \".txt\"\n",
    "            print(os.path.join(root, file_name)) #Displays the name of all the semi-structured text files\n",
    "            count = count + 1 # Increments the count after reading each file\n",
    "print(f\"\\nNumber of Semi-structured Files: {count}\") #Dispays the number of available files in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Exploration\n",
    "\n",
    "#### Displays the first 100 words from each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in ('.'): # Current directory\n",
    "    for each_file_present in os.listdir('.'): # For each available file in the current directory\n",
    "        if re.match('.*\\.txt', each_file_present): # Uses regular expression to match only .txt files\n",
    "            with open(os.path.join(files, each_file_present), \"r\", encoding='utf-8') as file_open: # Opens the file for reading with utf-8 as its encoding\n",
    "                line = file_open.readline().strip() # Reads the file line by line\n",
    "                print(line[0:100]) # Displays first 100 words of the current file\n",
    "            file_open.close() # Closes the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Extraction and Parsing\n",
    "\n",
    "Extracting all the `required` information from the available `semi-structured` text files and parsing the collected data into an `XML` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file  = open('dhrruvtokas.xml', \"w\", encoding=\"utf-8\") # Writes the extracted data into an XM file\n",
    "write_file.truncate(0) # Will remove anything that already exists in the XML file\n",
    "write_file.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\") # Writes the starting tag into the the XML file, it is the top marking line for the XML file, specifies type of encoding and version of the XML file\n",
    "write_file.write(\"\\n<data>\") # Writes the <data> tag for the XML file, <data> tag is opened, all the body information will be recorded in between <data> and </data> tags \n",
    "\n",
    "print (\"XML Formatted Information:\") # A label for the output\n",
    "print(\"\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\") # Top marking line for the XML dataset which will be displayed here using a print function, just for displaying the output\n",
    "print(\"<data>\")  # Top marking line for the XML dataset which will be displayed here using a print function, just for displaying the output\n",
    "\n",
    "for file in ('.'): # Current directory\n",
    "    for each_file in os.listdir('.'): # For each file in the current working directory\n",
    "        if re.match('.*\\.txt', each_file): # If extension of the file matches with .txt (if it is a text file)\n",
    "            with open(os.path.join(file, each_file), 'r', encoding='utf-8') as open_file: # Will open the file and specify its encoding as 'utf-8'\n",
    "                line = open_file.readline().strip() # Reads the whole file line by line\n",
    "                pattern_date = r'((19|20)(00[0-9]|1[0-9]|2[0-9])(-|/|)(0?[1-9]|1[0-2])-(00[0-9]|1[0-9]|2[0-9)]|3[0-1]))'\n",
    "                find_pattern_date = re.search(pattern_date, line) # Looks for a pattern (all the matching dates will be retrieved) that matches the specified pattern or regular expression\n",
    "                pattern_id = r'\\d{19}' # Specifies a pattern or regular expression for extracting all the tweet ids, each tweet id is a 19 digit number\n",
    "                find_pattern_id = re.search(pattern_id, line) # Looks for a pattern (all the matching tweet ids will be retrieved) that matches the specified pattern or regular expression\n",
    "                pattern_text = r'(?<=\"text\":\").*?(?=\")' # Specifies a pattern or regular expression for extracting all the recorded tweets (texts), so anything that comes between \"text\": and \" will be extracted as a tweet\n",
    "                find_pattern_text = re.search(pattern_text, line) # Looks for a pattern (all the the tweets which come between \"text\": and \")\n",
    "                check_english = langid.classify(find_pattern_text.group(0))[0] # Uses the langid.classify() function to verify whether the twets generated are in english or not, thi function on call will return the type of test language that is being used (en/fr/it where en = english) along with the matching factor (A real number). But here it will only return th matching factor as I have restricted the returning output by specifying langid.classify()[1]\n",
    "                if find_pattern_date is not None: # If there's a match for the date\n",
    "                    if find_pattern_id is not None: # If there's a match for the tweet id\n",
    "                        if find_pattern_text is not None: # If there's a match for the tweets (texts)\n",
    "                            if find_pattern_id not in hold_repeated: # If tweet is is unique, the current tweet id will be matched with the tweet ids present in the hold_repeated variable\n",
    "                                if (check_english==\"en\"): # If the matching factor (real number returned) is less than 0 or if it is negative then the tweet is in english or it contains english words and if the tweet does not contain any english words then it will not be extracted for the main XML file\n",
    "                                    hold_repeated.add(find_pattern_id.group(0)) # If it is a unique tweet id then it weill be added to set function inside the hold repeated_variable\n",
    "                                    write_file.write(f\"\\n<tweets date=\\\"{find_pattern_date.group(0)}\\\">\") # Writes the matching dates into an XML file\n",
    "                                    write_file.write(f\"\\n<tweet id=\\\"{find_pattern_id.group(0)}\\\">{find_pattern_text.group(0)}</tweet>\") # Writes the matching tweet ids into an XML file along with the matching tweets (texts)\n",
    "                                    print(f\"<tweets date=\\\"{find_pattern_date.group(0)}\\\">\") # Matched dates for the XML file will be displayed here using a print function\n",
    "                                    print(f\"<tweet id=\\\"{find_pattern_id.group(0)}\\\">{find_pattern_text.group(0)}</tweet>\") # Matched tweet ids and tweets (texts) will be displayed here using a print function\n",
    "                else: # If there's no match then pass\n",
    "                    pass\n",
    "            open_file.close() # Closes the file\n",
    "            \n",
    "print(\"</tweets>\") # </tweets> closing tag will be displayed here using a print function, just for displaying the output\n",
    "print(\"</data>\") # </data> closing tag will be displayed here using a print function, just for displaying the output            \n",
    "\n",
    "write_file.write(\"\\n</tweets>\") # Writes the </tweets> tag to the XML file, </tweets> tag is closed, <tweets> tag for the XML file\n",
    "write_file.write(\"\\n</data>\") # Writes the </data> tag to the XML file, </data> tag is closed for the XML file\n",
    "write_file.close() # Closes the XML file after writing is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Recorded Tweet ID(s)\n",
    "\n",
    "The following section will display the all those `tweet-ids` which have been retrieved from the dataset as well as the `total` number number of individual `tweet-ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Tweet IDs: \",len(hold_repeated)) # Displays the number of recorded unique tweet ids\n",
    "print(\"\\nList of Tweet IDs:\") # Label for tweet IDs\n",
    "print(\"\\n\", hold_repeated) # Displays all the unique tweet ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Reviewing The XML File\n",
    "\n",
    "The entire content documented in the `XML` file will be displayed in the following section \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_review = open('dhrruvtokas.xml', \"r\", encoding=\"utf-8\") # Opens the XML file fo reading its content, for the final review\n",
    "print(open_review.read()) # Displays the content of XML file \n",
    "open_review.close() # Closes the XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
